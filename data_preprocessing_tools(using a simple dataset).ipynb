{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of data_preprocessing_tools.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HasanMdKamrul/DataScienceProject/blob/main/data_preprocessing_tools(using%20a%20simple%20dataset).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37puETfgRzzg"
      },
      "source": [
        "# Data Preprocessing Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoRP98MpR-qj"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ERzT20Tofi"
      },
      "source": [
        "import numpy as np # ML models will expect some numpy models as their inputs\n",
        "import matplotlib.pyplot as plt # Allows to plot charts and graphs\n",
        "import pandas as pd # import datasets + also create the matrix of features and the dependent variable vector"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RopL7tUZSQkT"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV610vQoV3tN"
      },
      "source": [
        "dataset = pd.read_csv('Data.csv')\n",
        "x = dataset.iloc[:,:-1].values # Matrix of features X (Independent Variables)\n",
        "y = dataset.iloc[:,-1].values # Dependent Variables Y (Desicions)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW-Ce1W7cfP4",
        "outputId": "2b48881f-3f81-4712-d297-7ac10ccb445a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_pn1LH8ciEJ",
        "outputId": "e045aee3-a374-4a96-a60c-047cd6d94bd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfKXNxlSabC"
      },
      "source": [
        "## Taking care of missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhhB0Ktte1qW"
      },
      "source": [
        "from sklearn.impute import SimpleImputer # Class for missing data handelling as  their mean value replacement standard\n",
        "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean') # instance of that class and 2 arguments(1.find the missing values 2.What with we're gonna change here mean)\n",
        "imputer.fit(x[:,1:3]) # fit method find the avg or mean of the following columns\n",
        "x[:,1:3] = imputer.transform(x[:,1:3]) # Replace the missing values of the data and store in a updated one"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49vWeIPmjzk4",
        "outputId": "d26b7c3d-2991-4bf0-8463-3345d357a66c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x) # Printing of the updated values at the missing places"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CriG6VzVSjcK"
      },
      "source": [
        "## Encoding categorical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhSpdQWeSsFh"
      },
      "source": [
        "### Encoding the Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W9idM4Tu6b8"
      },
      "source": [
        "from sklearn.compose import ColumnTransformer # this class will encode the string into binary vector here our country column\n",
        "from sklearn.preprocessing import OneHotEncoder # To encode we need a class to binary vector which is OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])], remainder='passthrough') # instance = (2 arguments 1. transformers(1.What type of method we applied to chage our data,here encoder2.Who will do that job done, OneHotEncoder(),3.Where we applied the encoding, column[0]) 2. remainder(passthrough means it won't affect our other independet varriables and regardless keep them as it is))\n",
        "x = np.array(ct.fit_transform(x)) # Not do like privious step bcoz now we have ColumnTransformer class which has built in fit transform method in it, which done  the job of fitting the changed values at our updated matrix of features or the independent part"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1C3LP46wABc",
        "outputId": "123b6add-0542-45fa-c365-5ae350a1f009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXh8oVSITIc6"
      },
      "source": [
        "### Encoding the Dependent Variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHvwFlL22YeI"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder # This encoder used when decision depends on 2 values like yes/no, OneHotEncoder has 3 values or more values to deal\n",
        "le = LabelEncoder()# No arguments in the () bcoz it has only coloum to work with\n",
        "y = le.fit_transform(y) # dependent var dosen't need to be np array thats why we avoided unlike the prior one"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3GWiEaJ24-b",
        "outputId": "fdb08119-6207-4b41-bce3-4a150a2b94de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb_vcgm3qZKW"
      },
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aAJBAJ3kgPQ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split #train_test method will split the train and test sets\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.2, random_state=1) # train_test method returns 4 variable independent train(80% data), independent test(20%data), dependent train and test sets"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC27nyzXlJTs",
        "outputId": "aaa249d8-bd96-4f2e-debe-31bd8a3cff02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x_train)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHlagZ8KlJ0o",
        "outputId": "f549cd23-dfd6-43ee-f92f-504980f68ec5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x_test)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfdQhAztlJ78",
        "outputId": "bde872b5-93a3-4095-ce59-890766f1652f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_JglW_tlKJm",
        "outputId": "bf4c5812-6707-4850-90d0-6ab4dfa68c65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y_test)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIIkJFw_mCMy"
      },
      "source": [
        "# Why we have to do the feature scalling after the split?\n",
        "# if we do the feature scalling before the split the information will leak from the train set to test set\n",
        "# Where the test set is completely new observations depending on the train set data or results \n",
        "# so before tain the train set if we feature scale it will affect our test set predictions or observations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpGqbS4TqkIR"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-7YUynKml41"
      },
      "source": [
        "# Why we feature scale?\n",
        "# Becoz we don't want to dominate few important features to be dominated by the others which has no significant play in our machine learning model\n",
        "# It is not necessary for all the ML models but to avoid dominanace one feature to another we need to scale all the features in the same scale\n",
        "# Two method to feature scaling 1.Standardisation 2. Normalisation\n",
        "# Normalisation is for normal distribution of data but Standardisation will fit in all the situations(recommended to use it)\n",
        "# What we'll do here we first apply the standardisation with the x_train data for our x_train and we'll use the same x_train data for out x_test as well, So we'll not use the x_test data to calculate the x_test standardisation\n",
        "# Standardisation, Xstand = x-mean(x/standard deviation(x)--->root square varriance). ---> data varried between -3 to +3\n",
        "# Xnorm = x-min(x)/max(x)-min(x) ----> data varried between 0 and 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnEG9fhTq3BV"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler # StandardScaler class will do the standardisation\n",
        "sc = StandardScaler() # Creating instance with no arguments bcoz only x values to take see the formula\n",
        "x_train[:,3:] = sc.fit_transform(x_train[:,3:])# fit will give the mean and standard deviation but tranform will provide the whole equation result\n",
        "x_test[:,3:] = sc.transform(x_test[:,3:]) # only the transform used in test set while the mean and stndard deviation is used from the x_train set"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2EzSMzQwYln",
        "outputId": "68a48d19-a26d-4771-cf73-a8616649a8b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x_train)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwQKWtnewnnr",
        "outputId": "094f46be-84bb-453b-95fd-3768312f06d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x_test)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}